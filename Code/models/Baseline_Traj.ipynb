{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Traj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NVsPhIdQW8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cb7c8b-8b45-436e-da26-1286a1adc914"
      },
      "source": [
        "# needed libraries\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import Flatten, Reshape, Dropout, BatchNormalization, Activation, LeakyReLU\n",
        "\n",
        "# utilities\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "print(gpu_available)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "aW82hZgkZco1",
        "outputId": "8d854b12-2091-47b4-e487-b14bd60f68b9"
      },
      "source": [
        "from google.colab import drive\n",
        "drive._mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-5a06ee438a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server, ephemeral)\u001b[0m\n\u001b[1;32m    294\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_metadata_server\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed: invalid oauth code'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed: invalid oauth code"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install pickle5\n",
        "import pickle5 as pickle\n",
        "\n",
        "# store processed data in pkl files\n",
        "def save_pkl_data(data, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(data, file, pickle.HIGHEST_PROTOCOL)\n",
        "        print(\"data stored succesfully to: \", filename)\n",
        "\n",
        "\n",
        "# read processed data in pkl files\n",
        "def load_pkl_data(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        data = pickle.load(file)\n",
        "    return data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trLxIedJysba",
        "outputId": "011f683c-4ade-40d8-cc85-9e38b16436f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pickle5\n",
            "  Downloading pickle5-0.0.12-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (256 kB)\n",
            "\u001b[?25l\r\u001b[K     |█▎                              | 10 kB 22.1 MB/s eta 0:00:01\r\u001b[K     |██▋                             | 20 kB 29.5 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 30 kB 23.0 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 40 kB 18.6 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 51 kB 14.5 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 61 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 71 kB 12.5 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 81 kB 13.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 92 kB 14.0 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 102 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 112 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 122 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 133 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 143 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 153 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 163 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 174 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 184 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 194 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 204 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 215 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 225 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 235 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 245 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 12.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 256 kB 12.2 MB/s \n",
            "\u001b[?25hInstalling collected packages: pickle5\n",
            "Successfully installed pickle5-0.0.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cubes = load_pkl_data('nusc_inps.pkl') "
      ],
      "metadata": {
        "id": "x7yfHGkBzd3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Masking"
      ],
      "metadata": {
        "id": "Nux0ZPWEkULg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_look_ahead_mask(input):\n",
        "  input_shape = list(input.shape)[:-1]\n",
        "  input_shape.insert(-1, input_shape[-1])\n",
        "  input_shape.insert(1, 1)\n",
        "  mask = 1 - tf.linalg.band_part(tf.ones(input_shape), -1, 0)\n",
        "  return mask"
      ],
      "metadata": {
        "id": "aP3nXwLZkSuk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adapt_spatial_mask(mask):\n",
        "  return mask[np.newaxis, : , np.newaxis, : ]         #(1 (head), seq, 1 (neighbor), neighbors) to broadcast when doing addition in the attention layer"
      ],
      "metadata": {
        "id": "XiTKrQl_TImn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def adapt_seq_mask(mask):\n",
        "  return mask[np.newaxis, np.newaxis, np.newaxis, : ]   #(1 (head), 1(neighbors), 1(seq), seq)"
      ],
      "metadata": {
        "id": "8Eix_Y853Y3c"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Positional Encoding"
      ],
      "metadata": {
        "id": "u7MKnzFQkVv5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IKXnqHHQsL3"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oItHcbouQaKR"
      },
      "source": [
        "def positional_encoding(max_position, d_model):\n",
        "  angle_rads = get_angles(np.arange(max_position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention"
      ],
      "metadata": {
        "id": "p-EO4nKanJbP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmod-0KwX4Yj"
      },
      "source": [
        "def ScaledDotProduct(Q, K, V, mask=None):\n",
        "    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "\n",
        "    # compute attention \n",
        "    KT = tf.transpose(K, [0, 1, 2, 4, 3])                 \n",
        "    attention = tf.matmul(Q, KT)/tf.sqrt(dk)\n",
        "\n",
        "    # mask if necessary\n",
        "    if mask is not None:\n",
        "      #print(attention.shape)\n",
        "      attention += (mask * -1e9)\n",
        "\n",
        "    # compute values and weighted sum of their attention\n",
        "    weights = tf.nn.softmax(attention, axis=-1)\n",
        "    output = tf.matmul(weights, V)\n",
        "\n",
        "    return output, weights "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEUnKAzX7b4I"
      },
      "source": [
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "  def __init__(self, dk=256, num_heads=8):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    \n",
        "    # params\n",
        "    self.num_heads = num_heads\n",
        "    self.dk = dk\n",
        "    self.dk_by_head = dk//num_heads\n",
        "\n",
        "    # layers\n",
        "    self.WQ = keras.layers.Dense(dk)\n",
        "    self.WK = keras.layers.Dense(dk)\n",
        "    self.WV = keras.layers.Dense(dk)\n",
        "    self.dense = keras.layers.Dense(dk)\n",
        "    \n",
        "  def splitheads(self, x):\n",
        "    batch_size, seq_length = x.shape[0:2]\n",
        "\n",
        "    # spliting the heads done by reshaping last dimension\n",
        "    x = tf.reshape(x, (batch_size, seq_length, -1, self.num_heads, self.dk_by_head))      #(batch, seq, neighbors, head, features_by_head)\n",
        "    return tf.transpose(x, (0, 3, 1, 2, 4))                                               #(batch, head, seq, neighbors, features_by_head)\n",
        "\n",
        "  def call(self, q, k, v, mask=None):\n",
        "    batch_size, seq_length = q.shape[0:2]\n",
        "\n",
        "    # projections\n",
        "    q = self.WQ(q)\n",
        "    k = self.WK(v)\n",
        "    v = self.WV(k)\n",
        "\n",
        "    # split heads\n",
        "    q = self.splitheads(q)\n",
        "    k = self.splitheads(k)\n",
        "    v = self.splitheads(v)\n",
        "\n",
        "    # compute attention and merge heads\n",
        "    attn_output, attention = ScaledDotProduct(q, k, v, mask)                              #(batch, head, seq, neighbors, features_by_head)\n",
        "    attn_output = tf.transpose(attn_output,  (0, 2, 3, 1, 4))                             #(batch, seq, neighbors, head, features_by_head)\n",
        "    concat_output = tf.reshape(attn_output, (batch_size, seq_length, -1, self.dk))        #(batch, seq, neighbors, features)\n",
        "    output = self.dense(concat_output)\n",
        "\n",
        "    return output, attention\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Architecture"
      ],
      "metadata": {
        "id": "l7Ab42_LnOar"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_FFPE5rHQeI"
      },
      "source": [
        "def get_ffn(d_model, hidden_size, act_func='relu'):\n",
        "  return keras.models.Sequential([\n",
        "                                  keras.layers.Dense(hidden_size, activation=act_func),\n",
        "                                  keras.layers.Dense(d_model)\n",
        "  ], name='SEQ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwYCmqYIEs5K"
      },
      "source": [
        "class EncoderLayer(keras.layers.Layer):\n",
        "  def __init__(self, dk=256, num_heads=8, hidden_layer_size=256, use_dropout=True, drop_rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    # params\n",
        "    self.use_dropout = use_dropout\n",
        "\n",
        "    # layers\n",
        "    self.MH = MultiHeadAttention(dk, num_heads)\n",
        "    self.ffn = get_ffn(dk, dk, 'relu')\n",
        "    self.normLayer1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normLayer2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = keras.layers.Dropout(drop_rate)\n",
        "    self.dropout2 = keras.layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, x, training, mask):\n",
        "    # multihead attention\n",
        "    attn_output, _ = self.MH(x, x, x, mask)\n",
        "\n",
        "    # dropout layer\n",
        "    if self.use_dropout and training:\n",
        "      attn_output = self.dropout1(attn_output)\n",
        "    \n",
        "    # normalization and feed forward layers\n",
        "    z = self.normLayer1(x + attn_output)\n",
        "    output = self.ffn(z)\n",
        "\n",
        "    # dropout layer\n",
        "    if self.use_dropout and training:\n",
        "      output = self.dropout2(output)\n",
        "    \n",
        "    # normalization layer\n",
        "    output = self.normLayer2(z + output)\n",
        "\n",
        "    return output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ekRaNEvMjo"
      },
      "source": [
        "sample_encoder_layer = EncoderLayer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJHE0M97vihK",
        "outputId": "56b71cef-e5ac-401c-f2e4-4bc81fb03ef9"
      },
      "source": [
        "samp_inp = tf.random.uniform((3, 20, 6, 256))\n",
        "out = sample_encoder_layer(samp_inp, True, None)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 20, 6, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJhiiUwqAmvJ"
      },
      "source": [
        "class DecoderLayer(keras.layers.Layer):\n",
        "  def __init__(self, dk=256, num_heads=8, hidden_layer=256, use_dropout=True, drop_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    #params\n",
        "    self.use_dropout = use_dropout\n",
        "\n",
        "    # layers\n",
        "    self.SAMH = MultiHeadAttention(dk, num_heads)\n",
        "    self.EDMH = MultiHeadAttention(dk, num_heads)\n",
        "    self.ffn = get_ffn(dk, hidden_layer)\n",
        "\n",
        "    self.normLayer1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normLayer2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normLayer3 = keras.layers.LayerNormalization(epsilon=1e-6)\\\n",
        "\n",
        "    self.dropout1 = keras.layers.Dropout(drop_rate)\n",
        "    self.dropout2 = keras.layers.Dropout(drop_rate)\n",
        "    self.dropout3 = keras.layers.Dropout(drop_rate)\n",
        "  \n",
        "  def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
        "\n",
        "    # self attention computation\n",
        "    self_attn_out, self_attn = self.SAMH(x, x, x, look_ahead_mask)\n",
        "\n",
        "    if self.use_dropout and training:\n",
        "      self_attn_out = self.dropout1(self_attn_out)\n",
        "    \n",
        "    z = self.normLayer1(x + self_attn_out) \n",
        "\n",
        "    # encoder decoder computation\n",
        "    enc_dec_out, enc_dec_attn = self.EDMH(z, enc_output, enc_output, padding_mask)\n",
        "\n",
        "    if self.use_dropout and training:\n",
        "      enc_dec_out = self.dropout2(enc_dec_out)\n",
        "    \n",
        "    z = self.normLayer2(z + enc_dec_out)\n",
        "\n",
        "    # feed forward computation\n",
        "    output = self.ffn(z)\n",
        "\n",
        "    if self.use_dropout and training:\n",
        "      output = self.dropout3(output)\n",
        "    \n",
        "    output = self.normLayer3(z + output)\n",
        "\n",
        "    return output, self_attn, enc_dec_attn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXerYKS7vJBC"
      },
      "source": [
        "sample_decoder_layer = DecoderLayer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y19KWa5EzF1u",
        "outputId": "8127c674-ea51-4f4c-952a-34a547d18228"
      },
      "source": [
        "dec_inp = tf.random.uniform((3, 20, 6, 256))\n",
        "out2 = sample_decoder_layer(dec_inp, out, True, None, None)\n",
        "out2[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 20, 6, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jBOAtR05aGn"
      },
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, features_size, max_size, dk_model=256, num_heads=8, num_encoders=6, \n",
        "               enc_hidden_size=256, use_pos_emb=True, use_dropout=True, drop_rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    # params\n",
        "    self.dk_model = dk_model\n",
        "    self.max_size = max_size\n",
        "    self.use_dropout = use_dropout\n",
        "    self.use_pos_emb = use_pos_emb\n",
        "    self.enc_hidden_size = enc_hidden_size\n",
        "    self.num_encoders = num_encoders\n",
        "\n",
        "    # layers\n",
        "    #self.embedding = keras.layers.Embedding(features_size, dk_model)\n",
        "    self.embedding = keras.layers.Dense(dk_model)\n",
        "    self.encoders_stack = [EncoderLayer(dk_model, num_heads, enc_hidden_size, use_dropout, drop_rate) for _ in range(num_encoders)]\n",
        "    self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "  \n",
        "  def call(self, x, padding_mask, training):\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.dk_model, tf.float32))\n",
        "\n",
        "    if self.use_pos_emb:\n",
        "      x += positional_encoding(self.max_size, self.dk_model)\n",
        "    \n",
        "    if self.use_dropout and training:\n",
        "      x = self.dropout(x)\n",
        "    \n",
        "    for encoder_layer in self.encoders_stack:\n",
        "      x = encoder_layer(x, training, padding_mask)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mPf0yfTBQmf",
        "outputId": "2b03d983-b381-47f3-fa44-6bec338c9dbc"
      },
      "source": [
        "samp_inp = tf.random.uniform((3, 6, 20, 256))\n",
        "encoder = Encoder(256, 20, 256)\n",
        "out = encoder(samp_inp, None, True)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HI\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 6, 20, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 172
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxqvjK00I7q_"
      },
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, features_size, max_size, dk_model=256, num_heads=8, num_decoders=6, \n",
        "               dec_hidden_size=256, use_pos_emb=True, use_dropout=True, drop_rate=0.1):\n",
        "    \n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # params\n",
        "    self.dk_model = dk_model\n",
        "    self.max_size = max_size\n",
        "    self.use_dropout = use_dropout\n",
        "    self.use_pos_emb = use_pos_emb\n",
        "    self.dec_hidden_size = dec_hidden_size\n",
        "    self.num_decoders = num_decoders\n",
        "\n",
        "    # layers\n",
        "    self.embedding = keras.layers.Dense(dk_model)\n",
        "    self.decoders_stack = [DecoderLayer(dk_model, num_heads, dec_hidden_size, use_dropout, drop_rate) for _ in range(num_decoders)]\n",
        "    self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, x, enc_output, look_ahead_mask, padding_mask, training):\n",
        "    #print(x)\n",
        "    x = self.embedding(x)\n",
        "    #print(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.dk_model, tf.float32))\n",
        "    if self.use_pos_emb:\n",
        "      x += positional_encoding(self.max_size, self.dk_model)\n",
        "    \n",
        "    #print(x)\n",
        "    if self.use_dropout and training:\n",
        "      x = self.dropout(x)\n",
        "    \n",
        "    for decoder_layer in self.decoders_stack:\n",
        "      x, attn1, attn2, = decoder_layer(x, enc_output, training, look_ahead_mask, padding_mask)\n",
        "    \n",
        "    return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TJsP0rtQf83",
        "outputId": "5eabc61f-5230-4180-cfb5-5fe5c004c98b"
      },
      "source": [
        "samp_inp = tf.random.uniform((3, 6, 20, 256))\n",
        "decoder = Decoder(256, 20, 256)\n",
        "out2 = decoder(samp_inp, out, None, None, True)\n",
        "out2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HI\n",
            "xs:  (3, 6, 20, 256)\n",
            "pe:  (1, 20, 256)\n",
            "xs:  (3, 6, 20, 256)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 6, 20, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 192
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gQwvAEa6JJ"
      },
      "source": [
        "class STTransformer(keras.Model):\n",
        "  def __init__(self, features_size, max_seq_size, max_neighbors_size, \n",
        "               sp_dk=256, sp_enc_heads=8, sp_dec_heads=8, sp_num_encoders=6, sp_num_decoders=6, \n",
        "               tm_dk=256, tm_enc_heads=8, tm_dec_heads=8, tm_num_encoders=6, tm_num_decoders=6, \n",
        "               dec_hidden_size=256, use_dropout=True, drop_rate=0.1):\n",
        "    \n",
        "    super(STTransformer, self).__init__()\n",
        "\n",
        "    # layers\n",
        "    self.sp_encoder = Encoder(features_size, max_neighbors_size, sp_dk, use_pos_emb=False)\n",
        "    self.sp_decoder = Decoder(features_size, max_neighbors_size, sp_dk, use_pos_emb=False)\n",
        "    self.tm_encoder = Encoder(features_size, max_seq_size, tm_dk)\n",
        "    self.tm_decoder = Decoder(features_size, max_seq_size, tm_dk)\n",
        "    self.linear = tf.keras.layers.Dense(3, name='Linear_Trans')\n",
        "\n",
        "    \n",
        "  def call(self, inputs, training):\n",
        "    inp, inp_masks, seq_inp_masks, targets, tar_masks, seq_tar_masks = inputs\n",
        "    \n",
        "    sp_enc_out = self.sp_encoder(inp,  inp_masks, training)                               #(batch, seq, neighbors, <spatial attn features>)\n",
        "    out = tf.transpose(sp_enc_out, [0, 2, 1, 3])                                          #(batch, neighbors, seq, <spatial attn features>)\n",
        "    tm_enc_out = self.tm_encoder(out, seq_inp_masks, training)                            #(batch, neighbots, seq, <time attn features>)\n",
        "    \n",
        "    # decode time\n",
        "    targets = tf.transpose(targets, [0, 2, 1, 3])                                         #(batch, neighbors, seq, features)\n",
        "    look_mask = get_look_ahead_mask(targets)\n",
        "    tm_dec_out = self.tm_decoder(targets, tm_enc_out, look_mask, seq_tar_masks, training) \n",
        "    out2 = tf.transpose(tm_dec_out, [0, 2, 1, 3])                                         #(batch, seq, neighbors, features)\n",
        "    sp_dec_out = self.sp_decoder(out2, sp_enc_out, None, tar_masks, training)\n",
        "    \n",
        "    # linear projection\n",
        "    output = self.linear(sp_dec_out)\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def buildDataSet(input, batch_size):\n",
        "  input_ds = tf.data.Dataset.from_tensor_slices([x[0].astype(np.float32) for x in input])\n",
        "  inpMask_ds = tf.data.Dataset.from_tensor_slices([ adapt_spatial_mask(x[1].astype(np.float32)) for x in input])\n",
        "  seq_inpMask_ds = tf.data.Dataset.from_tensor_slices([adapt_seq_mask(x[2].astype(np.float32)) for x in input])\n",
        "\n",
        "  target_ds = tf.data.Dataset.from_tensor_slices([x[3].astype(np.float32) for x in input])\n",
        "  tarMask_ds = tf.data.Dataset.from_tensor_slices([adapt_spatial_mask(x[4].astype(np.float32)) for x in input])\n",
        "  seq_tarMask_ds = tf.data.Dataset.from_tensor_slices([adapt_seq_mask(x[5].astype(np.float32)) for x in input])\n",
        "\n",
        "  dataset = tf.data.Dataset.zip((input_ds, inpMask_ds, seq_inpMask_ds, target_ds, tarMask_ds, seq_tarMask_ds))\n",
        "  dataset = dataset.shuffle(100)\n",
        "  dataset = dataset.batch(batch_size)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "N9OyGWldeJ9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 8"
      ],
      "metadata": {
        "id": "7_3uLD4kxq-H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = buildDataSet(cubes, BATCH_SIZE)"
      ],
      "metadata": {
        "id": "N0fnkEQk0h3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
        "  def __init__(self, d_model, warmup_steps=4000):\n",
        "    super(CustomSchedule, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.d_model = tf.cast(self.d_model, tf.float32)\n",
        "\n",
        "    self.warmup_steps = warmup_steps\n",
        "\n",
        "  def __call__(self, step):\n",
        "    arg1 = tf.math.rsqrt(step)\n",
        "    arg2 = step * (self.warmup_steps ** -1.5)\n",
        "\n",
        "    return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
      ],
      "metadata": {
        "id": "YXOX26yP0E2e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = CustomSchedule(16)\n",
        "optimizer = optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98,\n",
        "                                     epsilon=1e-9)"
      ],
      "metadata": {
        "id": "h2KxT0Zcy-hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "feat_size = 32\n",
        "seq_size = 8\n",
        "neigh_size = 10\n",
        "dk = 16\n",
        "n_heads = 4\n",
        "model = STTransformer(feat_size, seq_size, neigh_size, \n",
        "                      sp_dk=dk, sp_enc_heads=n_heads, sp_dec_heads=n_heads,\n",
        "                      tm_dk=dk, tm_enc_heads=n_heads, tm_dec_heads=n_heads,\n",
        "                      sp_num_encoders=3, sp_num_decoders=3, tm_num_encoders=3, tm_num_decoders=3)"
      ],
      "metadata": {
        "id": "BwHu9EMU1rDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss_object = tf.keras.losses.MeanSquaredError()"
      ],
      "metadata": {
        "id": "-KumylNDtudp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_function(real, pred, seq_mask_array, neighbors_mask):\n",
        "  seq_mask_array = 1 - seq_mask_array\n",
        "  neighbors_mask = 1 - neighbors_mask\n",
        "\n",
        "  seq_mask_array = seq_mask_array[:, :, np.newaxis, np.newaxis]\n",
        "  neighbors_mask = neighbors_mask[:, :, :, np.newaxis]\n",
        "\n",
        "  pred_masked = pred * seq_mask_array\n",
        "  pred_masked = pred_masked * neighbors_mask\n",
        "  loss_ = loss_object(real, pred_masked)\n",
        "\n",
        "  return loss_"
      ],
      "metadata": {
        "id": "Rk5t6J9xlLeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@tf.function\n",
        "def train_step(zipped_input, losses):\n",
        "\n",
        "  # all model inputs\n",
        "  inputs = zipped_input[0]\n",
        "  neigh_inp_masks = zipped_input[1]\n",
        "  seq_inp_masks = zipped_input[2]\n",
        "\n",
        "  # targets\n",
        "  tar = zipped_input[3]\n",
        "  neigh_tar_masks = zipped_input[4]\n",
        "  seq_tar_masks = zipped_input[5]\n",
        "\n",
        "  seq_out_masks = tf.squeeze(seq_tar_masks)\n",
        "  neigh_out_masks = tf.squeeze(neigh_tar_masks)\n",
        "\n",
        "  \n",
        "  # get only x, y, and rotation\n",
        "  targets = tar[:, :, :, :3]                                            \n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = model((inputs, neigh_inp_masks, seq_inp_masks, tar, neigh_tar_masks, seq_tar_masks), training=True)\n",
        "    loss = loss_function(targets, predictions, seq_out_masks, neigh_tar_masks)\n",
        "\n",
        "  #print(predictions)\n",
        "  print('loss: ', loss)\n",
        "  losses.append(loss)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "  return losses, loss"
      ],
      "metadata": {
        "id": "UIDyvCrwiSpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_step(zipped_input):\n",
        "  # all model inputs\n",
        "  inputs = zipped_input[0]\n",
        "  neigh_inp_masks = zipped_input[1]\n",
        "  seq_inp_masks = zipped_input[2]\n",
        "\n",
        "  # targets\n",
        "  tar = zipped_input[3]\n",
        "  neigh_tar_masks = zipped_input[4]\n",
        "  seq_tar_masks = zipped_input[5]\n",
        "\n",
        "  seq_out_masks = tf.squeeze(seq_tar_masks)\n",
        "  neigh_out_masks = tf.squeeze(neigh_tar_masks)\n",
        "\n",
        "  targets = tf.transpose(tar[:, :, :, :2], [0, 2, 1, 3])     \n",
        "  preds = model((inputs, neigh_inp_masks, seq_inp_masks, tar, neigh_tar_masks, seq_tar_masks), training=False)\n",
        "\n",
        "  neigh_out_masks = 1 - neigh_out_masks\n",
        "  seq_out_masks = 1 - seq_out_masks\n",
        "  seq_out_masks = seq_out_masks[:, :, np.newaxis, np.newaxis]\n",
        "\n",
        "  # masking\n",
        "  preds = preds * seq_out_masks\n",
        "  preds = preds * neigh_out_masks[:, :, :, np.newaxis]\n",
        "  preds = preds[:, :, :, :2]\n",
        "\n",
        "  # sequence with feats dimension\n",
        "  preds = tf.transpose(preds, [0, 2, 1, 3])\n",
        "\n",
        "  # reshape to remove batch\n",
        "  targets = tf.reshape(targets, (-1, 8, 2))\n",
        "  preds = tf.reshape(preds, (-1, 8, 2))\n",
        "  \n",
        "  return ADE(targets.numpy(), preds.numpy())"
      ],
      "metadata": {
        "id": "63--K4-q2xFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "1OW2kykQ7iMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_file = '/content/drive/MyDrive/Colab_Notebooks/pesos_transformer'\n",
        "\n",
        "final_checkpoint = tf.train.Checkpoint(model=model)"
      ],
      "metadata": {
        "id": "S9dfpIPWwzBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5vkOw8TsLDJo"
      },
      "source": [
        "# Load Model If necessary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGX2KcdWLF6u",
        "outputId": "2f34c827-f4c7-4ebb-eb94-8e1a62636a72"
      },
      "source": [
        "final_checkpoint.read(final_file).assert_consumed()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f6a873cf2d0>"
            ]
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(10):\n",
        "  print('epoch: ', epoch)\n",
        "  losses = []\n",
        "  for batch in dataset:\n",
        "    losses, loss = train_step(batch, losses)\n",
        "    if np.isnan(loss.numpy()):\n",
        "      break;\n",
        "  \n",
        "  l_ade = []\n",
        "  for batch in dataset:\n",
        "    ade = eval_step(batch)\n",
        "    l_ade.append(ade)\n",
        "\n",
        "  print('ade: ', np.mean(np.array(l_ade)))  \n",
        "\n",
        "  print(\"avg loss\", tf.reduce_mean(losses)) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMQdHyG02AeR",
        "outputId": "4cc2dab5-64fa-47f3-b23f-5b17fd1eaf71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:  0\n",
            "loss:  tf.Tensor(176.25484, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(118.58099, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(195.1319, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(79.62045, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(357.97433, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(127.219955, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(143.39894, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(156.43394, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(136.0303, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(132.96176, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(178.79048, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(198.95853, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(188.69234, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(166.93082, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(111.72363, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(223.94678, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(228.81241, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(208.09143, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(269.48404, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(230.85925, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(159.29259, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(219.80461, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(284.74774, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(346.34344, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(328.99088, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(553.72314, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(273.07797, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(356.53522, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(343.70526, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(383.43802, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(278.70175, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(527.51355, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(209.86592, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(272.15973, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(254.01855, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(294.83435, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(305.6137, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(219.27124, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(283.05948, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(623.93115, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(382.68317, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(274.59833, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(234.22989, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(206.06235, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(294.00873, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(230.45532, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(130.01173, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(209.79636, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(164.2244, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(356.3631, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(427.52264, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(306.43274, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(257.49298, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(314.9209, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(485.64038, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(340.21213, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(123.86455, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(415.88885, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(776.45374, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(96.06539, shape=(), dtype=float32)\n",
            "ade:  14.244612\n",
            "avg loss tf.Tensor(269.59088, shape=(), dtype=float32)\n",
            "epoch:  1\n",
            "loss:  tf.Tensor(202.69531, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(104.83699, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(362.44363, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(102.76951, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(108.94216, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(125.675766, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(143.28673, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(93.571785, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(54.60215, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(84.97014, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(343.20435, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(220.89673, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(175.34877, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(37.071312, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(183.39133, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(54.824303, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(101.047264, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(309.57745, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(200.12404, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(303.30103, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(219.75366, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(157.43623, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(239.93213, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(246.00166, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(420.79404, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(268.57065, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(529.6593, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(133.09, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(468.76382, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(330.6179, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(557.8646, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(319.96347, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(296.69724, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(348.1922, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(275.3628, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(308.90936, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(312.46918, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(193.47775, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(470.55396, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(400.237, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(259.09985, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(201.17548, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(374.63113, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(193.37607, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(484.28897, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(330.6591, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(247.39847, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(270.61917, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(63.792915, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(257.7264, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(416.3583, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(541.6728, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(140.03452, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(254.21591, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(401.89777, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(681.3501, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(346.2331, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(257.95917, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(277.35016, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(464.24957, shape=(), dtype=float32)\n",
            "ade:  14.297504\n",
            "avg loss tf.Tensor(271.25027, shape=(), dtype=float32)\n",
            "epoch:  2\n",
            "loss:  tf.Tensor(273.10522, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(196.80058, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(123.185326, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(224.701, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(121.26167, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(111.03621, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(100.19368, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(80.20804, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(152.80547, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(226.76736, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(124.69371, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(142.97238, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(264.96674, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(206.4813, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(98.68197, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(119.72727, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(141.6444, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(283.71448, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(225.96626, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(130.39832, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(134.85277, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(212.87517, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(324.37946, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(211.44963, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(413.06308, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(407.4258, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(380.51404, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(298.5473, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(261.92032, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(258.39496, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(492.9419, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(291.0918, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(374.69928, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(464.2788, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(458.82208, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(249.76733, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(183.647, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(225.89934, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(749.7667, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(231.99614, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(354.98782, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(291.19592, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(212.36118, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(226.11653, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(139.30405, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(261.01776, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(220.35173, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(260.98346, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(423.12555, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(214.30437, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(180.90878, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(357.37054, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(263.43604, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(310.30692, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(390.78397, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(632.38055, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(468.56046, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(230.64145, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(322.6615, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(418.27988, shape=(), dtype=float32)\n",
            "ade:  14.280722\n",
            "avg loss tf.Tensor(269.2454, shape=(), dtype=float32)\n",
            "epoch:  3\n",
            "loss:  tf.Tensor(140.35379, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(162.0148, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(141.90346, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(95.34109, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(200.61406, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(151.7515, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(129.99515, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(101.22667, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(182.2795, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(143.83513, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(422.35385, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(204.17976, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(108.44524, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(196.65903, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(229.89958, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(66.49658, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(123.69845, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(213.73528, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(161.23175, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(263.99765, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(190.73175, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(213.66605, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(210.05806, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(172.46828, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(554.5858, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(365.67072, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(366.80563, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(153.60294, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(380.96786, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(291.29807, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(265.7691, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(293.49017, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(444.56635, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(391.2244, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(292.84, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(357.1459, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(147.31564, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(310.09845, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(194.48589, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(414.80478, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(437.5161, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(90.78009, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(272.4849, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(431.23926, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(406.03256, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(273.2518, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(380.53564, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(330.83453, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(210.19046, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(201.64233, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(409.71442, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(163.92589, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(269.9578, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(326.7979, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(297.8415, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(497.01227, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(318.87262, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(220.7736, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(670.71466, shape=(), dtype=float32)\n",
            "loss:  tf.Tensor(883.5928, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_checkpoint.write(final_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "T2JVKEaoxyUk",
        "outputId": "c933af85-e901-40f4-cce7-c827dca8e2d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/MyDrive/Colab_Notebooks/pesos_transformer'"
            ]
          },
          "metadata": {},
          "execution_count": 90
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ld = list(dataset)"
      ],
      "metadata": {
        "id": "a78xh9Ibyi_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = ld[0][0]\n",
        "mask = ld[0][1]\n",
        "s_mask = ld[0][2]\n",
        "tar = ld[0][3]\n",
        "t_mask = ld[0][4]\n",
        "st_mask = ld[0][5]\n",
        "seq_mask = ld[0][6]\n",
        "neig_mask = ld[0][7]\n",
        "\n",
        "# divide input as the trajectory input, and target (basically past and future to predict) \n",
        "out = model((inputs, mask, s_mask, tar, t_mask, st_mask), training=False)"
      ],
      "metadata": {
        "id": "7EfAfchgyq7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def ADE(real, pred):\n",
        "    diff_sq = (real - pred)**2\n",
        "    diff_sq = np.sum(diff_sq, axis=2)\n",
        "    diff_sq = np.sqrt(diff_sq)\n",
        "    mean_diff = np.mean(diff_sq)\n",
        "    return mean_diff"
      ],
      "metadata": {
        "id": "DdAzAs9WsKyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar[0, :, :, :3].shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KNUL8Ql7ztE2",
        "outputId": "731e4bbf-b455-4a5e-ae46-d78536627865"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([8, 10, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "neigh_mask = 1-neig_mask\n",
        "pred = out[0] * neigh_mask[0][:, :, np.newaxis]"
      ],
      "metadata": {
        "id": "cTAg0YO_zmOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tar_t = tf.transpose(tar[0,:,:, :3], [1, 0, 2])[:, :, :2]\n",
        "pred_t = tf.transpose(pred, [1,0,2])[:, :, :2]"
      ],
      "metadata": {
        "id": "V5mBC2JOsTfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ADE(tar_t.numpy(), pred_t.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJ9HKvSjtCoM",
        "outputId": "63e31e56-4886-45f8-d12b-376dab6b2d0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6.1653643"
            ]
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing functions"
      ],
      "metadata": {
        "id": "7fnl4A2rpBQv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(cubes)):\n",
        "  for j in range(len(cubes[i][0])):\n",
        "    for k in range(len(cubes[i][0][j])):\n",
        "      for l in range(len(cubes[i][0][j][k])):\n",
        "        if np.isnan(cubes[i][0][j][k][l]):\n",
        "          cubes[i][0][j][k][l] = 0.0      \n"
      ],
      "metadata": {
        "id": "0kov85YG8tVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_inps = [x[3] for x in cubes]\n",
        "for inp in all_inps:\n",
        "  for face in inp:\n",
        "    for row in face:\n",
        "      for el in row:\n",
        "        if np.isnan(el):\n",
        "          print('WHAAAAT')"
      ],
      "metadata": {
        "id": "i2yjv0GHqo0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.arange(10)[:, np.newaxis]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NS-_o2cMlZJ3",
        "outputId": "df15b9a2-cac7-4ccc-85db-efd08aa41b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [2],\n",
              "       [3],\n",
              "       [4],\n",
              "       [5],\n",
              "       [6],\n",
              "       [7],\n",
              "       [8],\n",
              "       [9]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = tf.constant(np.arange(3 * 4 * 3 * 5 * 5)) + 1    \n",
        "t = tf.reshape(t, (3, 4, 3, 5, 5))             #(batch, head, seq, N, N)\n",
        "t = tf.cast(t, tf.float32)\n",
        "t2 = np.random.choice([0, 1], (3, 3, 5)) * 0.5"
      ],
      "metadata": {
        "id": "v0gPWcILo_po"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t2 = tf.reshape(t2, (3, 1, 3, 1, 5))          # (batch, 1, seq, 1, N)\n",
        "t2 = tf.cast(t2, tf.float32)"
      ],
      "metadata": {
        "id": "6IIzQ0yfpn7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nP96rZJ2qmSG",
        "outputId": "db500925-f633-4f9b-8cb5-32822eda9bf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 1, 3, 1, 5), dtype=float32, numpy=\n",
              "array([[[[[0. , 0. , 0. , 0.5, 0. ]],\n",
              "\n",
              "         [[0. , 0.5, 0.5, 0.5, 0.5]],\n",
              "\n",
              "         [[0. , 0.5, 0. , 0.5, 0. ]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[0.5, 0. , 0.5, 0. , 0. ]],\n",
              "\n",
              "         [[0.5, 0. , 0. , 0. , 0.5]],\n",
              "\n",
              "         [[0. , 0. , 0.5, 0.5, 0.5]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[0. , 0. , 0.5, 0. , 0. ]],\n",
              "\n",
              "         [[0. , 0. , 0. , 0.5, 0. ]],\n",
              "\n",
              "         [[0.5, 0.5, 0.5, 0. , 0. ]]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t + t2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "il7hgzCmqokY",
        "outputId": "ad8395a6-b210-4e3b-976a-e5693dcdad32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(3, 4, 3, 5, 5), dtype=float32, numpy=\n",
              "array([[[[[  1. ,   2. ,   3. ,   4.5,   5. ],\n",
              "          [  6. ,   7. ,   8. ,   9.5,  10. ],\n",
              "          [ 11. ,  12. ,  13. ,  14.5,  15. ],\n",
              "          [ 16. ,  17. ,  18. ,  19.5,  20. ],\n",
              "          [ 21. ,  22. ,  23. ,  24.5,  25. ]],\n",
              "\n",
              "         [[ 26. ,  27.5,  28.5,  29.5,  30.5],\n",
              "          [ 31. ,  32.5,  33.5,  34.5,  35.5],\n",
              "          [ 36. ,  37.5,  38.5,  39.5,  40.5],\n",
              "          [ 41. ,  42.5,  43.5,  44.5,  45.5],\n",
              "          [ 46. ,  47.5,  48.5,  49.5,  50.5]],\n",
              "\n",
              "         [[ 51. ,  52.5,  53. ,  54.5,  55. ],\n",
              "          [ 56. ,  57.5,  58. ,  59.5,  60. ],\n",
              "          [ 61. ,  62.5,  63. ,  64.5,  65. ],\n",
              "          [ 66. ,  67.5,  68. ,  69.5,  70. ],\n",
              "          [ 71. ,  72.5,  73. ,  74.5,  75. ]]],\n",
              "\n",
              "\n",
              "        [[[ 76. ,  77. ,  78. ,  79.5,  80. ],\n",
              "          [ 81. ,  82. ,  83. ,  84.5,  85. ],\n",
              "          [ 86. ,  87. ,  88. ,  89.5,  90. ],\n",
              "          [ 91. ,  92. ,  93. ,  94.5,  95. ],\n",
              "          [ 96. ,  97. ,  98. ,  99.5, 100. ]],\n",
              "\n",
              "         [[101. , 102.5, 103.5, 104.5, 105.5],\n",
              "          [106. , 107.5, 108.5, 109.5, 110.5],\n",
              "          [111. , 112.5, 113.5, 114.5, 115.5],\n",
              "          [116. , 117.5, 118.5, 119.5, 120.5],\n",
              "          [121. , 122.5, 123.5, 124.5, 125.5]],\n",
              "\n",
              "         [[126. , 127.5, 128. , 129.5, 130. ],\n",
              "          [131. , 132.5, 133. , 134.5, 135. ],\n",
              "          [136. , 137.5, 138. , 139.5, 140. ],\n",
              "          [141. , 142.5, 143. , 144.5, 145. ],\n",
              "          [146. , 147.5, 148. , 149.5, 150. ]]],\n",
              "\n",
              "\n",
              "        [[[151. , 152. , 153. , 154.5, 155. ],\n",
              "          [156. , 157. , 158. , 159.5, 160. ],\n",
              "          [161. , 162. , 163. , 164.5, 165. ],\n",
              "          [166. , 167. , 168. , 169.5, 170. ],\n",
              "          [171. , 172. , 173. , 174.5, 175. ]],\n",
              "\n",
              "         [[176. , 177.5, 178.5, 179.5, 180.5],\n",
              "          [181. , 182.5, 183.5, 184.5, 185.5],\n",
              "          [186. , 187.5, 188.5, 189.5, 190.5],\n",
              "          [191. , 192.5, 193.5, 194.5, 195.5],\n",
              "          [196. , 197.5, 198.5, 199.5, 200.5]],\n",
              "\n",
              "         [[201. , 202.5, 203. , 204.5, 205. ],\n",
              "          [206. , 207.5, 208. , 209.5, 210. ],\n",
              "          [211. , 212.5, 213. , 214.5, 215. ],\n",
              "          [216. , 217.5, 218. , 219.5, 220. ],\n",
              "          [221. , 222.5, 223. , 224.5, 225. ]]],\n",
              "\n",
              "\n",
              "        [[[226. , 227. , 228. , 229.5, 230. ],\n",
              "          [231. , 232. , 233. , 234.5, 235. ],\n",
              "          [236. , 237. , 238. , 239.5, 240. ],\n",
              "          [241. , 242. , 243. , 244.5, 245. ],\n",
              "          [246. , 247. , 248. , 249.5, 250. ]],\n",
              "\n",
              "         [[251. , 252.5, 253.5, 254.5, 255.5],\n",
              "          [256. , 257.5, 258.5, 259.5, 260.5],\n",
              "          [261. , 262.5, 263.5, 264.5, 265.5],\n",
              "          [266. , 267.5, 268.5, 269.5, 270.5],\n",
              "          [271. , 272.5, 273.5, 274.5, 275.5]],\n",
              "\n",
              "         [[276. , 277.5, 278. , 279.5, 280. ],\n",
              "          [281. , 282.5, 283. , 284.5, 285. ],\n",
              "          [286. , 287.5, 288. , 289.5, 290. ],\n",
              "          [291. , 292.5, 293. , 294.5, 295. ],\n",
              "          [296. , 297.5, 298. , 299.5, 300. ]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[301.5, 302. , 303.5, 304. , 305. ],\n",
              "          [306.5, 307. , 308.5, 309. , 310. ],\n",
              "          [311.5, 312. , 313.5, 314. , 315. ],\n",
              "          [316.5, 317. , 318.5, 319. , 320. ],\n",
              "          [321.5, 322. , 323.5, 324. , 325. ]],\n",
              "\n",
              "         [[326.5, 327. , 328. , 329. , 330.5],\n",
              "          [331.5, 332. , 333. , 334. , 335.5],\n",
              "          [336.5, 337. , 338. , 339. , 340.5],\n",
              "          [341.5, 342. , 343. , 344. , 345.5],\n",
              "          [346.5, 347. , 348. , 349. , 350.5]],\n",
              "\n",
              "         [[351. , 352. , 353.5, 354.5, 355.5],\n",
              "          [356. , 357. , 358.5, 359.5, 360.5],\n",
              "          [361. , 362. , 363.5, 364.5, 365.5],\n",
              "          [366. , 367. , 368.5, 369.5, 370.5],\n",
              "          [371. , 372. , 373.5, 374.5, 375.5]]],\n",
              "\n",
              "\n",
              "        [[[376.5, 377. , 378.5, 379. , 380. ],\n",
              "          [381.5, 382. , 383.5, 384. , 385. ],\n",
              "          [386.5, 387. , 388.5, 389. , 390. ],\n",
              "          [391.5, 392. , 393.5, 394. , 395. ],\n",
              "          [396.5, 397. , 398.5, 399. , 400. ]],\n",
              "\n",
              "         [[401.5, 402. , 403. , 404. , 405.5],\n",
              "          [406.5, 407. , 408. , 409. , 410.5],\n",
              "          [411.5, 412. , 413. , 414. , 415.5],\n",
              "          [416.5, 417. , 418. , 419. , 420.5],\n",
              "          [421.5, 422. , 423. , 424. , 425.5]],\n",
              "\n",
              "         [[426. , 427. , 428.5, 429.5, 430.5],\n",
              "          [431. , 432. , 433.5, 434.5, 435.5],\n",
              "          [436. , 437. , 438.5, 439.5, 440.5],\n",
              "          [441. , 442. , 443.5, 444.5, 445.5],\n",
              "          [446. , 447. , 448.5, 449.5, 450.5]]],\n",
              "\n",
              "\n",
              "        [[[451.5, 452. , 453.5, 454. , 455. ],\n",
              "          [456.5, 457. , 458.5, 459. , 460. ],\n",
              "          [461.5, 462. , 463.5, 464. , 465. ],\n",
              "          [466.5, 467. , 468.5, 469. , 470. ],\n",
              "          [471.5, 472. , 473.5, 474. , 475. ]],\n",
              "\n",
              "         [[476.5, 477. , 478. , 479. , 480.5],\n",
              "          [481.5, 482. , 483. , 484. , 485.5],\n",
              "          [486.5, 487. , 488. , 489. , 490.5],\n",
              "          [491.5, 492. , 493. , 494. , 495.5],\n",
              "          [496.5, 497. , 498. , 499. , 500.5]],\n",
              "\n",
              "         [[501. , 502. , 503.5, 504.5, 505.5],\n",
              "          [506. , 507. , 508.5, 509.5, 510.5],\n",
              "          [511. , 512. , 513.5, 514.5, 515.5],\n",
              "          [516. , 517. , 518.5, 519.5, 520.5],\n",
              "          [521. , 522. , 523.5, 524.5, 525.5]]],\n",
              "\n",
              "\n",
              "        [[[526.5, 527. , 528.5, 529. , 530. ],\n",
              "          [531.5, 532. , 533.5, 534. , 535. ],\n",
              "          [536.5, 537. , 538.5, 539. , 540. ],\n",
              "          [541.5, 542. , 543.5, 544. , 545. ],\n",
              "          [546.5, 547. , 548.5, 549. , 550. ]],\n",
              "\n",
              "         [[551.5, 552. , 553. , 554. , 555.5],\n",
              "          [556.5, 557. , 558. , 559. , 560.5],\n",
              "          [561.5, 562. , 563. , 564. , 565.5],\n",
              "          [566.5, 567. , 568. , 569. , 570.5],\n",
              "          [571.5, 572. , 573. , 574. , 575.5]],\n",
              "\n",
              "         [[576. , 577. , 578.5, 579.5, 580.5],\n",
              "          [581. , 582. , 583.5, 584.5, 585.5],\n",
              "          [586. , 587. , 588.5, 589.5, 590.5],\n",
              "          [591. , 592. , 593.5, 594.5, 595.5],\n",
              "          [596. , 597. , 598.5, 599.5, 600.5]]]],\n",
              "\n",
              "\n",
              "\n",
              "       [[[[601. , 602. , 603.5, 604. , 605. ],\n",
              "          [606. , 607. , 608.5, 609. , 610. ],\n",
              "          [611. , 612. , 613.5, 614. , 615. ],\n",
              "          [616. , 617. , 618.5, 619. , 620. ],\n",
              "          [621. , 622. , 623.5, 624. , 625. ]],\n",
              "\n",
              "         [[626. , 627. , 628. , 629.5, 630. ],\n",
              "          [631. , 632. , 633. , 634.5, 635. ],\n",
              "          [636. , 637. , 638. , 639.5, 640. ],\n",
              "          [641. , 642. , 643. , 644.5, 645. ],\n",
              "          [646. , 647. , 648. , 649.5, 650. ]],\n",
              "\n",
              "         [[651.5, 652.5, 653.5, 654. , 655. ],\n",
              "          [656.5, 657.5, 658.5, 659. , 660. ],\n",
              "          [661.5, 662.5, 663.5, 664. , 665. ],\n",
              "          [666.5, 667.5, 668.5, 669. , 670. ],\n",
              "          [671.5, 672.5, 673.5, 674. , 675. ]]],\n",
              "\n",
              "\n",
              "        [[[676. , 677. , 678.5, 679. , 680. ],\n",
              "          [681. , 682. , 683.5, 684. , 685. ],\n",
              "          [686. , 687. , 688.5, 689. , 690. ],\n",
              "          [691. , 692. , 693.5, 694. , 695. ],\n",
              "          [696. , 697. , 698.5, 699. , 700. ]],\n",
              "\n",
              "         [[701. , 702. , 703. , 704.5, 705. ],\n",
              "          [706. , 707. , 708. , 709.5, 710. ],\n",
              "          [711. , 712. , 713. , 714.5, 715. ],\n",
              "          [716. , 717. , 718. , 719.5, 720. ],\n",
              "          [721. , 722. , 723. , 724.5, 725. ]],\n",
              "\n",
              "         [[726.5, 727.5, 728.5, 729. , 730. ],\n",
              "          [731.5, 732.5, 733.5, 734. , 735. ],\n",
              "          [736.5, 737.5, 738.5, 739. , 740. ],\n",
              "          [741.5, 742.5, 743.5, 744. , 745. ],\n",
              "          [746.5, 747.5, 748.5, 749. , 750. ]]],\n",
              "\n",
              "\n",
              "        [[[751. , 752. , 753.5, 754. , 755. ],\n",
              "          [756. , 757. , 758.5, 759. , 760. ],\n",
              "          [761. , 762. , 763.5, 764. , 765. ],\n",
              "          [766. , 767. , 768.5, 769. , 770. ],\n",
              "          [771. , 772. , 773.5, 774. , 775. ]],\n",
              "\n",
              "         [[776. , 777. , 778. , 779.5, 780. ],\n",
              "          [781. , 782. , 783. , 784.5, 785. ],\n",
              "          [786. , 787. , 788. , 789.5, 790. ],\n",
              "          [791. , 792. , 793. , 794.5, 795. ],\n",
              "          [796. , 797. , 798. , 799.5, 800. ]],\n",
              "\n",
              "         [[801.5, 802.5, 803.5, 804. , 805. ],\n",
              "          [806.5, 807.5, 808.5, 809. , 810. ],\n",
              "          [811.5, 812.5, 813.5, 814. , 815. ],\n",
              "          [816.5, 817.5, 818.5, 819. , 820. ],\n",
              "          [821.5, 822.5, 823.5, 824. , 825. ]]],\n",
              "\n",
              "\n",
              "        [[[826. , 827. , 828.5, 829. , 830. ],\n",
              "          [831. , 832. , 833.5, 834. , 835. ],\n",
              "          [836. , 837. , 838.5, 839. , 840. ],\n",
              "          [841. , 842. , 843.5, 844. , 845. ],\n",
              "          [846. , 847. , 848.5, 849. , 850. ]],\n",
              "\n",
              "         [[851. , 852. , 853. , 854.5, 855. ],\n",
              "          [856. , 857. , 858. , 859.5, 860. ],\n",
              "          [861. , 862. , 863. , 864.5, 865. ],\n",
              "          [866. , 867. , 868. , 869.5, 870. ],\n",
              "          [871. , 872. , 873. , 874.5, 875. ]],\n",
              "\n",
              "         [[876.5, 877.5, 878.5, 879. , 880. ],\n",
              "          [881.5, 882.5, 883.5, 884. , 885. ],\n",
              "          [886.5, 887.5, 888.5, 889. , 890. ],\n",
              "          [891.5, 892.5, 893.5, 894. , 895. ],\n",
              "          [896.5, 897.5, 898.5, 899. , 900. ]]]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = np.random.choice([0, 1], size=(3, 5))"
      ],
      "metadata": {
        "id": "XfRj6BV9U-Ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mask"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Gy4I67yVG1X",
        "outputId": "1d9a1849-9433-40a0-be14-a3b6603de2ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 0, 0, 1],\n",
              "       [1, 1, 0, 0, 1],\n",
              "       [0, 1, 0, 1, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "adapt_spatial_mask(mask).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTMo8uEhVJVQ",
        "outputId": "b8d4d91e-d68e-4aff-f708-70e1ba6d4bcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 3, 1, 5)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = np.arange(4 * 5 * 2 * 3) + 1\n",
        "inp = np.reshape(inp, (4, 5, 2, 3))"
      ],
      "metadata": {
        "id": "qilHYpsLkrP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inp"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaV8cscxteg-",
        "outputId": "e1d790d3-7134-4614-8b6a-6cc62397061d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3,  4],\n",
              "       [ 5,  6,  7,  8],\n",
              "       [ 9, 10, 11, 12],\n",
              "       [13, 14, 15, 16]])"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mask = 1 - np.array([[0, 0, 0, 1, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 0], [0, 0, 0, 0, 1]])\n",
        "mask = mask[:, :, np.newaxis, np.newaxis]\n",
        "inp * mask"
      ],
      "metadata": {
        "id": "mWmSlyXwk-Gq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07d3f782-2229-48e9-e553-2bda8b52090c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[[  1,   2,   3],\n",
              "         [  4,   5,   6]],\n",
              "\n",
              "        [[  7,   8,   9],\n",
              "         [ 10,  11,  12]],\n",
              "\n",
              "        [[ 13,  14,  15],\n",
              "         [ 16,  17,  18]],\n",
              "\n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0]],\n",
              "\n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0]]],\n",
              "\n",
              "\n",
              "       [[[ 31,  32,  33],\n",
              "         [ 34,  35,  36]],\n",
              "\n",
              "        [[ 37,  38,  39],\n",
              "         [ 40,  41,  42]],\n",
              "\n",
              "        [[ 43,  44,  45],\n",
              "         [ 46,  47,  48]],\n",
              "\n",
              "        [[ 49,  50,  51],\n",
              "         [ 52,  53,  54]],\n",
              "\n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0]]],\n",
              "\n",
              "\n",
              "       [[[ 61,  62,  63],\n",
              "         [ 64,  65,  66]],\n",
              "\n",
              "        [[ 67,  68,  69],\n",
              "         [ 70,  71,  72]],\n",
              "\n",
              "        [[ 73,  74,  75],\n",
              "         [ 76,  77,  78]],\n",
              "\n",
              "        [[ 79,  80,  81],\n",
              "         [ 82,  83,  84]],\n",
              "\n",
              "        [[ 85,  86,  87],\n",
              "         [ 88,  89,  90]]],\n",
              "\n",
              "\n",
              "       [[[ 91,  92,  93],\n",
              "         [ 94,  95,  96]],\n",
              "\n",
              "        [[ 97,  98,  99],\n",
              "         [100, 101, 102]],\n",
              "\n",
              "        [[103, 104, 105],\n",
              "         [106, 107, 108]],\n",
              "\n",
              "        [[109, 110, 111],\n",
              "         [112, 113, 114]],\n",
              "\n",
              "        [[  0,   0,   0],\n",
              "         [  0,   0,   0]]]])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = np.array([[[1.0, 2.0, 3.0], [1.0, 1.0, 2.0]], [[1.0, 2.0, 3.0], [1.0, 1.0, 2.0]]])\n",
        "\n",
        "\n",
        "tar = np.array([[[2.0, 4.0, 2.0], [1.0, 3.0, 2.0]], [[2.0, 4.0, 2.0], [1.0, 3.0, 2.0]]])\n",
        "\n",
        "loss_object(inp,tar)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZuKYu0KX5Xv",
        "outputId": "1d6093c9-b407-40ba-c589-25faea176639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(), dtype=float64, numpy=1.6666667461395264>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sum((inp - tar)**2)/12"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0FkVV8llPXe",
        "outputId": "4d6af90a-a84b-4b09-e1a2-b70138cb4ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.6666666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgBtRMn5oozQ",
        "outputId": "3e6ac555-1ee6-4794-9333-d32b3d655c90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[2., 4., 2.],\n",
              "       [1., 3., 2.]])"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "5/3.0 + 4.0/3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xf4qQTqLo32s",
        "outputId": "41348555-45ef-4e19-b231-a0c0e944fe32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.0"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    }
  ]
}