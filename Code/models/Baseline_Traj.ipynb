{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Baseline_Traj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NVsPhIdQW8-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10363919-aa40-46dd-b00c-e8a2297ab468"
      },
      "source": [
        "# needed libraries\n",
        "import re\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Conv2DTranspose\n",
        "from tensorflow.keras.layers import Flatten, Reshape, Dropout, BatchNormalization, Activation, LeakyReLU\n",
        "\n",
        "# utilities\n",
        "import os\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import pathlib\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "gpu_available = tf.config.list_physical_devices('GPU')\n",
        "print(gpu_available)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0IKXnqHHQsL3"
      },
      "source": [
        "def get_angles(pos, i, d_model):\n",
        "  angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
        "  return pos * angle_rates"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oItHcbouQaKR"
      },
      "source": [
        "def positional_encoding(max_position, d_model):\n",
        "  angle_rads = get_angles(np.arange(max_position)[:, np.newaxis],\n",
        "                          np.arange(d_model)[np.newaxis, :],\n",
        "                          d_model)\n",
        "\n",
        "  # apply sin to even indices in the array; 2i\n",
        "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
        "\n",
        "  # apply cos to odd indices in the array; 2i+1\n",
        "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
        "\n",
        "  pos_encoding = angle_rads[np.newaxis, ...]\n",
        "\n",
        "  return tf.cast(pos_encoding, dtype=tf.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hmod-0KwX4Yj"
      },
      "source": [
        "def ScaledDotProduct(Q, K, V, mask=None):\n",
        "    dk = tf.cast(tf.shape(K)[-1], tf.float32)\n",
        "\n",
        "    # compute attention \n",
        "    KT = tf.transpose(K, [0, 1, 2, 4, 3])                 \n",
        "    attention = tf.matmul(Q, KT)/tf.sqrt(dk)\n",
        "\n",
        "    # mask if necessary\n",
        "    if mask is not None:\n",
        "      attention += (mask * -1e9)\n",
        "\n",
        "    # compute values and weighted sum of their attention\n",
        "    weights = tf.nn.softmax(attention, axis=-1)\n",
        "    output = tf.matmul(weights, V)\n",
        "\n",
        "    return output, weights "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEUnKAzX7b4I"
      },
      "source": [
        "class MultiHeadAttention(keras.layers.Layer):\n",
        "  def __init__(self, dk=256, num_heads=8):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    \n",
        "    # params\n",
        "    self.num_heads = num_heads\n",
        "    self.dk = dk\n",
        "    self.dk_by_head = dk//num_heads\n",
        "\n",
        "    # layers\n",
        "    self.WQ = keras.layers.Dense(dk, use_bias=False)\n",
        "    self.WK = keras.layers.Dense(dk, use_bias=False)\n",
        "    self.WV = keras.layers.Dense(dk, use_bias=False)\n",
        "    self.dense = keras.layers.Dense(dk, use_bias=False)\n",
        "    \n",
        "  def splitheads(self, x):\n",
        "    batch_size, seq_length = x.shape[0:2]\n",
        "\n",
        "    # spliting the heads done by reshaping last dimension\n",
        "    x = tf.reshape(x, (batch_size, seq_length, -1, self.num_heads, self.dk_by_head))      #(batch, seq, neighbors, head, features_by_head)\n",
        "    return tf.transpose(x, (0, 3, 1, 2, 4))                                               #(batch, head, seq, neighbors, features_by_head)\n",
        "\n",
        "  def call(self, q, k, v, mask=None):\n",
        "    batch_size, seq_length = q.shape[0:2]\n",
        "\n",
        "    # projections\n",
        "    q = self.WQ(q)\n",
        "    k = self.WK(v)\n",
        "    v = self.WV(k)\n",
        "\n",
        "    # split heads\n",
        "    q = self.splitheads(q)\n",
        "    k = self.splitheads(k)\n",
        "    v = self.splitheads(v)\n",
        "\n",
        "    # compute attention and merge heads\n",
        "    attn_output, attention = ScaledDotProduct(q, k, v, mask)                              #(batch, head, seq, neighbors, features_by_head)\n",
        "    attn_output = tf.transpose(attn_output,  (0, 2, 3, 1, 4))                                  #(batch, seq, neighbors, head, features_by_head)\n",
        "    concat_output = tf.reshape(attn_output, (batch_size, seq_length, -1, self.dk))        #(batch, seq, neighbors, features)\n",
        "    output = self.dense(concat_output)\n",
        "\n",
        "    return output, attention\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2_FFPE5rHQeI"
      },
      "source": [
        "def get_ffn(d_model, hidden_size, act_func='relu'):\n",
        "  return keras.models.Sequential([\n",
        "                                  keras.layers.Dense(hidden_size, activation=act_func),\n",
        "                                  keras.layers.Dense(d_model)\n",
        "  ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwYCmqYIEs5K"
      },
      "source": [
        "class EncoderLayer(keras.layers.Layer):\n",
        "  def __init__(self, dk=256, num_heads=8, hidden_layer_size=256, use_dropout=True, drop_rate=0.1):\n",
        "    super(EncoderLayer, self).__init__()\n",
        "    # params\n",
        "    self.use_dropout = use_dropout\n",
        "\n",
        "    # layers\n",
        "    self.MH = MultiHeadAttention(dk, num_heads)\n",
        "    self.ffn = get_ffn(dk, dk, 'relu')\n",
        "    self.normLayer1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normLayer2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.dropout1 = keras.layers.Dropout(drop_rate)\n",
        "    self.dropout2 = keras.layers.Dropout(drop_rate)\n",
        "\n",
        "  def call(self, x, training, mask=None):\n",
        "    # multihead attention\n",
        "    attn_output, _ = self.MH(x, x, x, mask)\n",
        "\n",
        "    # dropout layer\n",
        "    if self.use_dropout and training:\n",
        "      attn_output = self.dropout1(attn_output)\n",
        "    \n",
        "    # normalization and feed forward layers\n",
        "    z = self.normLayer1(x + attn_output)\n",
        "    output = self.ffn(z)\n",
        "\n",
        "    # dropout layer\n",
        "    if self.use_dropout and training:\n",
        "      output = self.dropout2(output)\n",
        "    \n",
        "    # normalization layer\n",
        "    output = self.normLayer2(z + output)\n",
        "\n",
        "    return output "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "98ekRaNEvMjo"
      },
      "source": [
        "sample_encoder_layer = EncoderLayer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sJHE0M97vihK",
        "outputId": "4fca1be7-b997-4f50-fb06-a0581843c02a"
      },
      "source": [
        "samp_inp = tf.random.uniform((3, 20, 6, 256))\n",
        "out = sample_encoder_layer(samp_inp, True)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 20, 6, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 178
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJhiiUwqAmvJ"
      },
      "source": [
        "class DecoderLayer(keras.layers.Layer):\n",
        "  def __init__(self, dk=256, num_heads=8, hidden_layer=256, use_dropout=True, drop_rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    #params\n",
        "    self.use_dropout = use_dropout\n",
        "\n",
        "    # layers\n",
        "    self.SAMH = MultiHeadAttention(dk, num_heads)\n",
        "    self.EDMH = MultiHeadAttention(dk, num_heads)\n",
        "    self.ffn = get_ffn(dk, hidden_layer)\n",
        "\n",
        "    self.normLayer1 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normLayer2 = keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.normLayer3 = keras.layers.LayerNormalization(epsilon=1e-6)\\\n",
        "\n",
        "    self.dropout1 = keras.layers.Dropout(drop_rate)\n",
        "    self.dropout2 = keras.layers.Dropout(drop_rate)\n",
        "    self.dropout3 = keras.layers.Dropout(drop_rate)\n",
        "  \n",
        "  def call(self, x, enc_output, training, mask=None):\n",
        "\n",
        "    # self attention computation\n",
        "    self_attn_out, self_attn = self.SAMH(x, x, x, mask)\n",
        "\n",
        "    if self.use_dropout and training:\n",
        "      self_attn_out = self.dropout1(self_attn_out)\n",
        "    \n",
        "    z = self.normLayer1(x + self_attn_out) \n",
        "\n",
        "    # encoder decoder computation\n",
        "    enc_dec_out, enc_dec_attn = self.EDMH(z, enc_output, enc_output)\n",
        "\n",
        "    if self.use_dropout and training:\n",
        "      enc_dec_out = self.dropout2(enc_dec_out)\n",
        "    \n",
        "    z = self.normLayer2(z + enc_dec_out)\n",
        "\n",
        "    # feed forward computation\n",
        "    output = self.ffn(z)\n",
        "\n",
        "    if self.use_dropout and training:\n",
        "      output = self.dropout3(output)\n",
        "    \n",
        "    output = self.normLayer3(z + output)\n",
        "\n",
        "    return output, self_attn, enc_dec_attn\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XXerYKS7vJBC"
      },
      "source": [
        "sample_decoder_layer = DecoderLayer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y19KWa5EzF1u",
        "outputId": "dc0cfb06-b1f3-4518-e027-08464f4495ca"
      },
      "source": [
        "dec_inp = tf.random.uniform((3, 20, 6, 256))\n",
        "out2 = sample_decoder_layer(dec_inp, out, True)\n",
        "out2[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 20, 6, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 188
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jBOAtR05aGn"
      },
      "source": [
        "class Encoder(keras.layers.Layer):\n",
        "  def __init__(self, features_size, max_size, dk_model=256, num_heads=8, num_encoders=6, \n",
        "               enc_hidden_size=256, use_pos_emb=True, use_dropout=True, drop_rate=0.1):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    # params\n",
        "    self.dk_model = dk_model\n",
        "    self.max_size = max_size\n",
        "    self.use_dropout = use_dropout\n",
        "    self.use_pos_emb = use_pos_emb\n",
        "    self.enc_hidden_size = enc_hidden_size\n",
        "    self.num_encoders = num_encoders\n",
        "\n",
        "    # layers\n",
        "    #self.embedding = keras.layers.Embedding(features_size, dk_model)\n",
        "    self.embedding = keras.layers.Dense(dk_model)\n",
        "    self.encoders_stack = [EncoderLayer(dk_model, num_heads, enc_hidden_size, use_dropout, drop_rate) for _ in range(num_encoders)]\n",
        "    self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "  \n",
        "  def call(self, x, training):\n",
        "\n",
        "    x = self.embedding(x)\n",
        "    x *= tf.math.sqrt(tf.cast(self.dk_model, tf.float32))\n",
        "\n",
        "    if self.use_pos_emb:\n",
        "      x += positional_encoding(self.max_size, self.dk_model)\n",
        "    \n",
        "    if self.use_dropout and training:\n",
        "      x = self.dropout(x)\n",
        "    \n",
        "    for encoder_layer in self.encoders_stack:\n",
        "      x = encoder_layer(x, training)\n",
        "    \n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mPf0yfTBQmf",
        "outputId": "83c2dee6-dfa1-4ddd-8b98-c2130051538d"
      },
      "source": [
        "samp_inp = tf.random.uniform((3, 6, 20, 256))\n",
        "encoder = Encoder(256, 20, 256)\n",
        "out = encoder(samp_inp, True)\n",
        "out.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 6, 20, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 237
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxqvjK00I7q_"
      },
      "source": [
        "class Decoder(keras.layers.Layer):\n",
        "  def __init__(self, features_size, max_size, dk_model=256, num_heads=8, num_decoders=6, \n",
        "               dec_hidden_size=256, use_pos_emb=True, use_dropout=True, drop_rate=0.1):\n",
        "    \n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # params\n",
        "    self.dk_model = dk_model\n",
        "    self.max_size = max_size\n",
        "    self.use_dropout = use_dropout\n",
        "    self.use_pos_emb = use_pos_emb\n",
        "    self.dec_hidden_size = dec_hidden_size\n",
        "    self.num_decoders = num_decoders\n",
        "\n",
        "    # layers\n",
        "    self.embedding = keras.layers.Dense(dk_model)\n",
        "    self.decoders_stack = [DecoderLayer(dk_model, num_heads, dec_hidden_size, use_dropout, drop_rate) for _ in range(num_decoders)]\n",
        "    self.dropout = tf.keras.layers.Dropout(drop_rate)\n",
        "\n",
        "    def call(self, x, enc_output, training):\n",
        "\n",
        "      x = self.embedding(x)\n",
        "      x *= tf.math.sqrt(tf.cast(self.dk_model, tf.float32))\n",
        "\n",
        "      if self.use_pos_emb:\n",
        "        x += positional_encoding(self.max_size, self.dk_model)\n",
        "      \n",
        "      if self.use_dropout and training:\n",
        "        x = self.dropout(x)\n",
        "      \n",
        "      for decoder_layer in self.decoders_stack:\n",
        "        x, attn1, attn2, = decoder_layer(x, enc_output, training)\n",
        "      \n",
        "      return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TJsP0rtQf83",
        "outputId": "81e286fb-0d95-4333-ad93-b1cc78164c06"
      },
      "source": [
        "samp_inp = tf.random.uniform((3, 6, 20, 256))\n",
        "decoder = Decoder(256, 20, 256)\n",
        "out2 = decoder(samp_inp, out, True)\n",
        "out2.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 6, 20, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 241
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9gQwvAEa6JJ"
      },
      "source": [
        "class STTransformer(keras.layers.Layer):\n",
        "  def __init__(self, features_size, max_seq_size, max_neighbors_size, \n",
        "               sp_dk=256, sp_enc_heads=8, sp_dec_heads=8, sp_num_encoders=6, sp_num_decoders=6, \n",
        "               tm_dk=256, tm_enc_heads=8, tm_dec_heads=8, tm_num_encoders=6, tm_num_decoders=6, \n",
        "               dec_hidden_size=256, use_dropout=True, drop_rate=0.1):\n",
        "    \n",
        "    super(STTransformer, self).__init__()\n",
        "\n",
        "    # layers\n",
        "    self.sp_encoder = Encoder(features_size, max_neighbors_size, sp_dk, use_pos_emb=False)\n",
        "    self.sp_decoder = Decoder(features_size, max_neighbors_size, sp_dk, use_pos_emb=False)\n",
        "    self.tm_encoder = Encoder(features_size, max_seq_size, tm_dk)\n",
        "    self.tm_decoder = Decoder(features_size, max_seq_size, tm_dk)\n",
        "\n",
        "    \n",
        "  def call(self, inputs, training):\n",
        "    x = inputs\n",
        "    sp_enc_out = self.sp_encoder(x, training)                               #(batch, seq, neighbors, <spatial attn features>)\n",
        "    out = tf.transpose(sp_enc_out, [0, 2, 1, 3])                            #(batch, neighbors, seq, <spatial attn features>)\n",
        "    tm_enc_out = self.tm_encoder(out, training)                             #(batch, neighbots, seq, <time attn features>)\n",
        "\n",
        "    # review shapes\n",
        "    #tm_dec_out = self.tm_decoder(targets, tm_enc_out, training)\n",
        "    #sp_dec_out = self.sp_decoder(tm_dec_out, ts_enc_out, training)\n",
        "    return tm_enc_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "capRr2Fl_XNw"
      },
      "source": [
        "model = STTransformer(100, 20, 6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMcNVIeu_3cq"
      },
      "source": [
        "input = tf.random.uniform((3, 20, 6, 256))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CmOgF0uAYta"
      },
      "source": [
        "o = model(input)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_cqvWb9jFpN",
        "outputId": "534ceb16-742f-4661-8634-ef85745e413e"
      },
      "source": [
        "o.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([3, 6, 20, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    }
  ]
}